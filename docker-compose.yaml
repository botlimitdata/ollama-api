version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    # Ollama will only listen on the internal Docker network.
    # It won't be directly exposed to the outside.
    volumes:
      - ollama_models:/root/.ollama
    # The Ollama API will not have its port mapped to the host.
    # The `api-gateway` service will call it via internal port 11434.
    # Optional: Use GPU if available.
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    environment:
      # Important: Ollama only allows access from the internal Docker network.
      - OLLAMA_ORIGINS=http://api-gateway:8000
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped

  api-gateway:
    build: ./api-gateway
    container_name: api-gateway
    depends_on:
      - ollama
    # This service will listen on the internal Docker network.
    # Nginx will forward external requests to it.
    environment:
      # Define a secret key for authentication.
      # This key must be changed to a random, secure value.
      - API_SECRET_KEY=your_secret_key_here
      # The address of Ollama within the internal Docker network.
      - OLLAMA_URL=http://ollama:11434
    restart: unless-stopped

  nginx:
    image: nginx:latest
    container_name: nginx-reverse-proxy
    depends_on:
      - api-gateway
    # Port 8000 will be exposed to the outside to receive requests from clients.
    # You can change this port if you wish.
    ports:
      - "8000:8000"
    # Mount the Nginx configuration file.
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    restart: unless-stopped

volumes:
  ollama_models: